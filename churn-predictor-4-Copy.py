# %%
import pandas as pd
import os

# Ki·ªÉm tra xem d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load v√†o ch∆∞a
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# %%
# C√†i ƒë·∫∑t phi√™n b·∫£n protobuf t∆∞∆°ng th√≠ch
!pip install protobuf==3.20.3

# %%
# Th√™m d·∫•u ch·∫•m than ƒë·ªÉ ch·∫°y l·ªánh h·ªá th·ªëng
!curl -I https://huggingface.co

# %% [markdown]
# # 1: New merge: Dataset g·ªëc + check th√™m c√≥ csv n√†o kh√¥ng

# %%
import pandas as pd
import os
import glob # Th∆∞ vi·ªán n√†y gi√∫p t√¨m file theo t√™n ƒëu√¥i

# =============================================================================
# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# =============================================================================

# 1. File g·ªëc c·ªë ƒë·ªãnh (Main History)
# ƒê√¢y l√† file d·ªØ li·ªáu l·ªãch s·ª≠ l·ªõn, √≠t thay ƒë·ªïi t√™n
MAIN_DATA_PATH = '/kaggle/input/olist-merged-dataset-2016-2017/olist_merged_2016_2017.csv'

# 2. Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu m·ªõi (Folder Update)
# B·∫°n ch·ªâ c·∫ßn tr·ªè ƒë·∫øn T√äN TH∆Ø M·ª§C dataset, kh√¥ng c·∫ßn t√™n file c·ª• th·ªÉ.
# V√≠ d·ª•: Khi b·∫°n Add Data b·ªô "Olist Updates", ƒë∆∞·ªùng d·∫´n th∆∞·ªùng l√†:
UPDATE_DIR = '/kaggle/input/olist-merged-dataset-2016-2017'

# =============================================================================
# LOGIC T·ª∞ ƒê·ªòNG QU√âT V√Ä G·ªòP (AUTO-SCAN TRIGGER)
# =============================================================================

print("--- B·∫ÆT ƒê·∫¶U QUY TR√åNH SMART LOAD ---")

# --- B∆∞·ªõc 1: Load d·ªØ li·ªáu g·ªëc ---
if os.path.exists(MAIN_DATA_PATH):
    print(f"‚úÖ ƒêang ƒë·ªçc d·ªØ li·ªáu g·ªëc t·ª´: {MAIN_DATA_PATH}")
    orders_features_df = pd.read_csv(MAIN_DATA_PATH)
    print(f"   -> K√≠ch th∆∞·ªõc ban ƒë·∫ßu: {orders_features_df.shape}")
else:
    # N·∫øu kh√¥ng t√¨m th·∫•y file g·ªëc, th·ª≠ t√¨m trong th∆∞ m·ª•c hi·ªán t·∫°i xem c√≥ file n√†o l·ªõn kh√¥ng
    print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file g·ªëc ƒë√≠ch danh. ƒêang t√¨m file CSV l·ªõn nh·∫•t trong input...")
    # (Logic d·ª± ph√≤ng n√†y gi√∫p b·∫°n ƒë·ª° ph·∫£i s·ª≠a path n·∫øu l·ª° ƒë·ªïi t√™n file g·ªëc)
    pass 

# --- B∆∞·ªõc 2: Qu√©t th∆∞ m·ª•c Update ƒë·ªÉ t√¨m file CSV l·∫° ---
# L·∫•y t·∫•t c·∫£ file .csv trong th∆∞ m·ª•c update
update_files = glob.glob(os.path.join(UPDATE_DIR, "*.csv"))
update_files = [f for f in update_files if os.path.abspath(f) != os.path.abspath(MAIN_DATA_PATH)]

if len(update_files) > 0:
    print(f"\nüöÄ PH√ÅT HI·ªÜN {len(update_files)} FILE D·ªÆ LI·ªÜU M·ªöI:")
    for file_path in update_files:
        try:
            new_data_df = pd.read_csv(file_path)
            orders_features_df = pd.concat([orders_features_df, new_data_df], ignore_index=True)
            print(f"   + ƒê√£ th√™m {new_data_df.shape[0]} d√≤ng t·ª´: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"   ‚ùå L·ªói file {file_path}: {e}")
else:
    print(f"\n‚ÑπÔ∏è Kh√¥ng c√≥ file m·ªõi.")

# =================================================================
# QUAN TR·ªåNG: LU√îN LU√îN X√ìA TR√ôNG L·∫∂P (D√π c√≥ file m·ªõi hay kh√¥ng)
# =================================================================
if 'order_item_id' in orders_features_df.columns:
    orders_features_df.drop_duplicates(subset=['order_id', 'order_item_id'], keep='last', inplace=True)
    print("ƒê√£ x√≥a tr√πng l·∫∑p d·ª±a tr√™n (Order ID + Item ID).")

# 2. Ho·∫∑c n·∫øu data ƒë√£ aggregate (kh√¥ng c√≥ item id), nh∆∞ng b·∫°n s·ª£ x√≥a nh·∫ßm d√≤ng kh√°c nhau:
# Ch·ªâ x√≥a khi d√≤ng ƒë√≥ TR√ôNG 100% t·∫•t c·∫£ c√°c c·ªôt
else:
    # B·ªè tham s·ªë subset ƒë·ªÉ so s√°nh to√†n b·ªô c√°c c·ªôt
    orders_features_df.drop_duplicates(keep='last', inplace=True) 
    print("ƒê√£ x√≥a c√°c d√≤ng tr√πng l·∫∑p ho√†n to√†n (Duplicate Rows).")

# =============================================================================
# CHU·∫®N H√ìA D·ªÆ LI·ªÜU (B·∫ÆT BU·ªòC)
# =============================================================================
print("\n--- CHU·∫®N H√ìA D·ªÆ LI·ªÜU ---")

date_cols = ['order_purchase_timestamp', 'order_delivered_customer_date', 'order_estimated_delivery_date']
for col in date_cols:
    if col in orders_features_df.columns:
        orders_features_df[col] = pd.to_datetime(orders_features_df[col], errors='coerce')

# 2. Sort l·∫°i
if 'order_purchase_timestamp' in orders_features_df.columns:
    orders_features_df.sort_values(by='order_purchase_timestamp', inplace=True)

# 3. T√°i t·∫°o c·ªôt thi·∫øu
if 'delivery_days' not in orders_features_df.columns:
    orders_features_df['delivery_days'] = (orders_features_df['order_delivered_customer_date'] - orders_features_df['order_purchase_timestamp']).dt.days
if 'delivery_delay_days' not in orders_features_df.columns:
    orders_features_df['delivery_delay_days'] = (orders_features_df['order_delivered_customer_date'] - orders_features_df['order_estimated_delivery_date']).dt.days
if 'num_items' not in orders_features_df.columns:
    orders_features_df['num_items'] = orders_features_df['order_item_id'].fillna(1) if 'order_item_id' in orders_features_df.columns else 1

# 4. Fill NaN
for col in ['delivery_days', 'delivery_delay_days', 'num_items', 'payment_value', 'review_score']:
    if col in orders_features_df.columns:
        orders_features_df[col] = orders_features_df[col].fillna(0)

print(f"\nüéâ Final Data Shape: {orders_features_df.shape}")

# %% [markdown]
# ## Ng√†y c·ªßa d·ªØ li·ªáu

# %%
import pandas as pd

# S·ª≠ d·ª•ng bi·∫øn 'orders_features_df' thay v√¨ 'df'
if 'orders_features_df' in locals() and isinstance(orders_features_df, pd.DataFrame) and 'order_purchase_timestamp' in orders_features_df.columns:
    # Convert to datetime if not already
    orders_features_df['order_purchase_timestamp'] = pd.to_datetime(orders_features_df['order_purchase_timestamp'], errors='coerce')

    # Find the earliest and latest timestamps
    earliest_date = orders_features_df['order_purchase_timestamp'].min()
    latest_date = orders_features_df['order_purchase_timestamp'].max()

    if pd.notnull(earliest_date) and pd.notnull(latest_date):
        print(f"B·ªô d·ªØ li·ªáu ch·ª©a c√°c ƒë∆°n h√†ng t·ª´ ng√†y: {earliest_date.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ƒê·∫øn ng√†y: {latest_date.strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print("C·ªôt th·ªùi gian kh√¥ng ch·ª©a d·ªØ li·ªáu h·ª£p l·ªá.")
else:
    print("Kh√¥ng t√¨m th·∫•y DataFrame 'orders_features_df' ho·∫∑c c·ªôt 'order_purchase_timestamp'.")

# %%
import pandas as pd
import numpy as np

# --- ƒêO·∫†N CODE N√ÄY ƒê·ªÇ T·∫†O L·∫†I C√ÅC C·ªòT B·ªä THI·∫æU ---
# Ch·∫°y ƒëo·∫°n n√†y ngay sau khi load orders_features_df v√† tr∆∞·ªõc khi ch·∫°y c√°c b∆∞·ªõc ti·∫øp theo

print("ƒêang t√°i t·∫°o c√°c c·ªôt ƒë·∫∑c tr∆∞ng (Features) b·ªã thi·∫øu...")

# 1. Chuy·ªÉn ƒë·ªïi c√°c c·ªôt th·ªùi gian sang ƒë·ªãnh d·∫°ng datetime (B·∫Øt bu·ªôc ƒë·ªÉ tr·ª´ ng√†y th√°ng)
# errors='coerce' s·∫Ω bi·∫øn c√°c gi√° tr·ªã l·ªói/r√°c th√†nh NaT (ƒë·ªÉ tr√°nh l·ªói RuntimeWarning ph√≠a tr√™n)
date_cols = ['order_purchase_timestamp', 'order_delivered_customer_date', 'order_estimated_delivery_date']
for col in date_cols:
    if col in orders_features_df.columns:
        orders_features_df[col] = pd.to_datetime(orders_features_df[col], errors='coerce')

# 2. T·∫°o c·ªôt 'delivery_days' (Th·ªùi gian giao h√†ng th·ª±c t·∫ø)
if 'delivery_days' not in orders_features_df.columns:
    orders_features_df['delivery_days'] = (orders_features_df['order_delivered_customer_date'] - orders_features_df['order_purchase_timestamp']).dt.days

# 3. T·∫°o c·ªôt 'delivery_delay_days' (ƒê·ªô tr·ªÖ so v·ªõi d·ª± ki·∫øn)
if 'delivery_delay_days' not in orders_features_df.columns:
    orders_features_df['delivery_delay_days'] = (orders_features_df['order_delivered_customer_date'] - orders_features_df['order_estimated_delivery_date']).dt.days

# 4. T·∫°o c·ªôt 'num_items' (S·ªë l∆∞·ª£ng s·∫£n ph·∫©m trong ƒë∆°n)
if 'num_items' not in orders_features_df.columns:
    # N·∫øu file c·ªßa b·∫°n c√≥ c·ªôt 'order_item_id', ta d√πng n√≥ ƒë·ªÉ ƒë·∫°i di·ªán s·ªë l∆∞·ª£ng (ho·∫∑c g√°n = 1 n·∫øu kh√¥ng c√≥ th√¥ng tin)
    if 'order_item_id' in orders_features_df.columns:
        # Gi·∫£ s·ª≠ dataset ƒë√£ merge, n·∫øu mu·ªën ch√≠nh x√°c tuy·ªát ƒë·ªëi c·∫ßn group l·∫°i, 
        # nh∆∞ng ƒë·ªÉ code ch·∫°y ƒë∆∞·ª£c ngay, ta c√≥ th·ªÉ fill t·∫°m ho·∫∑c d√πng logic ƒë∆°n gi·∫£n:
        orders_features_df['num_items'] = orders_features_df['order_item_id'].fillna(1)
    else:
        print("C·∫£nh b√°o: Kh√¥ng c√≥ c·ªôt order_item_id, g√°n num_items = 1")
        orders_features_df['num_items'] = 1

# 5. X·ª≠ l√Ω s·∫°ch d·ªØ li·ªáu (H·∫øt c·∫£nh b√°o ƒë·ªè)
# ƒêi·ªÅn 0 v√†o c√°c √¥ b·ªã tr·ªëng do t√≠nh to√°n ng√†y th√°ng (v√≠ d·ª• ƒë∆°n ch∆∞a giao xong)
cols_to_fix = ['delivery_days', 'delivery_delay_days', 'num_items', 'payment_value', 'review_score']
for col in cols_to_fix:
    if col in orders_features_df.columns:
        orders_features_df[col] = orders_features_df[col].fillna(0)

print("ƒê√£ t·∫°o xong c√°c c·ªôt thi·∫øu: delivery_days, delivery_delay_days, num_items")
print(f"Shape hi·ªán t·∫°i: {orders_features_df.shape}")

# %% [markdown]
# # labeling

# %%
import pandas as pd
import numpy as np

# --- GI·∫¢ S·ª¨ 'orders_features_df' L√Ä DATAFRAME C·∫§P ƒê·ªò ƒê∆†N H√ÄNG T·ª™ PH·∫¶N 1 ---
# orders_features_df = pd.read_csv(...)
# orders_features_df['order_purchase_timestamp'] = pd.to_datetime(...)

print("--- B·∫Øt ƒë·∫ßu Ph·∫ßn 2 (M·ªõi): G√°n nh√£n Churn d·ª±a tr√™n Ng∆∞·ª°ng ƒê·ªông ---")

# --- B∆Ø·ªöC 2.1: S·∫ÆP X·∫æP V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---
# S·∫Øp x·∫øp theo kh√°ch h√†ng v√† th·ªùi gian l√† b∆∞·ªõc quan tr·ªçng nh·∫•t
df_sorted = orders_features_df.sort_values(by=['customer_unique_id', 'order_purchase_timestamp'])

# --- B∆Ø·ªöC 2.2: T√çNH KHO·∫¢NG TH·ªúI GIAN MUA L·∫†I (REPURCHASE GAPS) ---
# 2.2.1. T√≠nh kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn mua c·ªßa c√πng m·ªôt kh√°ch h√†ng
print("T√≠nh to√°n personal_avg_gap...")
df_sorted['days_since_previous_purchase'] = df_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].diff().dt.days

# 2.2.2. T√≠nh kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn mua c·ªßa c√πng m·ªôt kh√°ch h√†ng TRONG C√ôNG m·ªôt danh m·ª•c
print("T√≠nh to√°n category_avg_gap...")
df_sorted['days_since_previous_purchase_in_category'] = df_sorted.groupby(['customer_unique_id', 'product_category_name_english'])['order_purchase_timestamp'].diff().dt.days

# --- B∆Ø·ªöC 2.3: T√çNH NG∆Ø·ª†NG C√Å NH√ÇN V√Ä NG∆Ø·ª†NG DANH M·ª§C ---
# 2.3.1. T√≠nh personal_avg_gap (th√≥i quen mua s·∫Øm c√° nh√¢n)
# S·ª≠ d·ª•ng transform('mean') ƒë·ªÉ t√≠nh trung b√¨nh v√† g√°n l·∫°i cho t·∫•t c·∫£ c√°c h√†ng c·ªßa kh√°ch h√†ng ƒë√≥
df_sorted['personal_avg_gap'] = df_sorted.groupby('customer_unique_id')['days_since_previous_purchase'].transform('mean')

# 2.3.2. T√≠nh category_avg_gap (th√≥i quen mua s·∫Øm theo danh m·ª•c)
# ·ªû ƒë√¢y ta t√≠nh trung b√¨nh to√†n c·ª•c c·ªßa danh m·ª•c, kh√¥ng ph·∫£i c·ªßa c√° nh√¢n trong danh m·ª•c
category_global_avg_gap = df_sorted.groupby('product_category_name_english')['days_since_previous_purchase_in_category'].mean().reset_index(name='category_avg_gap')
df_sorted = pd.merge(df_sorted, category_global_avg_gap, on='product_category_name_english', how='left')


# --- B∆Ø·ªöC 2.4: X·ª¨ L√ù GI√Å TR·ªä NaN (CHO KH√ÅCH H√ÄNG MUA 1 L·∫¶N) ---
print("X·ª≠ l√Ω gi√° tr·ªã NaN cho kh√°ch h√†ng mua 1 l·∫ßn...")
# ∆Ø·ªõc t√≠nh th√≥i quen cho nh·ªØng ng∆∞·ªùi ch∆∞a c√≥ l·ªãch s·ª≠ mua l·∫°i
# b·∫±ng ph√¢n v·ªã th·ª© 75 c·ªßa nh·ªØng ng∆∞·ªùi c√≥ l·ªãch s·ª≠.
# ƒê√¢y l√† m·ªôt gi·∫£ ƒë·ªãnh h·ª£p l√Ω: "m·ªôt kh√°ch h√†ng m·ªõi c√≥ th·ªÉ s·∫Ω h√†nh x·ª≠ gi·ªëng nh∆∞
# nh·ªØng kh√°ch h√†ng c√≥ xu h∆∞·ªõng mua l·∫°i ch·∫≠m h∆°n m·ªôt ch√∫t".
global_p75_customer_gap = df_sorted['days_since_previous_purchase'].quantile(0.75)
df_sorted['personal_avg_gap'] = df_sorted['personal_avg_gap'].fillna(global_p75_customer_gap)

global_p75_category_gap = df_sorted['days_since_previous_purchase_in_category'].quantile(0.75)
df_sorted['category_avg_gap'] = df_sorted['category_avg_gap'].fillna(global_p75_category_gap)


# --- B∆Ø·ªöC 2.5: T√çNH NG∆Ø·ª†NG R·ªúI B·ªé ƒê·ªòNG (HYBRID HORIZON) ---
print("T√≠nh to√°n ng∆∞·ª°ng r·ªùi b·ªè ƒë·ªông 'hybrid_horizon'...")
w1, w2 = 0.6, 0.4  # Tr·ªçng s·ªë cho th√≥i quen c√° nh√¢n v√† th√≥i quen danh m·ª•c
alpha = 1.5       # H·ªá s·ªë an to√†n (v√≠ d·ª•: ch·ªù g·∫•p 1.5 l·∫ßn th·ªùi gian th√¥ng th∆∞·ªùng)

df_sorted['hybrid_gap'] = (df_sorted['personal_avg_gap'] * w1 + df_sorted['category_avg_gap'] * w2)
df_sorted['hybrid_horizon'] = df_sorted['hybrid_gap'] * alpha


# --- B∆Ø·ªöC 2.6: G√ÅN NH√ÉN CHURN D·ª∞A TR√äN HYBRID HORIZON ---
print("G√°n nh√£n churn cu·ªëi c√πng...")
# T√≠nh ng√†y mua ti·∫øp theo ƒë·ªÉ so s√°nh
df_sorted['next_purchase_timestamp'] = df_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].shift(-1)
df_sorted['days_to_next_purchase'] = (df_sorted['next_purchase_timestamp'] - df_sorted['order_purchase_timestamp']).dt.days

dataset_end_date = df_sorted['order_purchase_timestamp'].max()

def assign_churn(row):
    # N·∫øu c√≥ l·∫ßn mua ti·∫øp theo (kh√¥ng ph·∫£i giao d·ªãch cu·ªëi)
    if pd.notnull(row['days_to_next_purchase']):
        # Churn n·∫øu kho·∫£ng th·ªùi gian ƒë·∫øn l·∫ßn mua ti·∫øp theo l·ªõn h∆°n ng∆∞·ª°ng
        return 1 if row['days_to_next_purchase'] > row['hybrid_horizon'] else 0
    # N·∫øu l√† l·∫ßn mua cu·ªëi c√πng
    else:
        days_since_last_purchase = (dataset_end_date - row['order_purchase_timestamp']).days
        # Churn n·∫øu th·ªùi gian ch·ªù k·ªÉ t·ª´ l·∫ßn mua cu·ªëi l·ªõn h∆°n ng∆∞·ª°ng
        return 1 if days_since_last_purchase > row['hybrid_horizon'] else 0

df_sorted['churn'] = df_sorted.apply(assign_churn, axis=1)


# --- B∆Ø·ªöC 2.7: D·ªåN D·∫∏P V√Ä HO√ÄN THI·ªÜN ---
# Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt v√† ƒë·ªïi t√™n 'df_sorted' th√†nh 'final_df' ƒë·ªÉ nh·∫•t qu√°n
final_df = df_sorted.copy()

# D·ªçn d·∫πp c√°c c·ªôt trung gian kh√¥ng c·∫ßn thi·∫øt cho c√°c b∆∞·ªõc sau
cols_to_drop = [
    'days_since_previous_purchase', 'days_since_previous_purchase_in_category',
    'next_purchase_timestamp', 'days_to_next_purchase', 'personal_avg_gap',
    'category_avg_gap', 'hybrid_gap', 'hybrid_horizon'
]
final_df.drop(columns=cols_to_drop, inplace=True, errors='ignore')


# --- KI·ªÇM TRA K·∫æT QU·∫¢ ---
print("\n--- K·∫æT TH√öC PH·∫¶N 2 (M·ªöI) ---")
print("Ph√¢n ph·ªëi nh√£n Churn m·ªõi:")
print(final_df['churn'].value_counts(normalize=True))

print("\nShape c·ªßa final_df m·ªõi:", final_df.shape)
print("\n5 d√≤ng cu·ªëi c·ªßa final_df ƒë·ªÉ ki·ªÉm tra:")
print(final_df[['customer_unique_id', 'order_purchase_timestamp', 'churn']].tail())

# %% [markdown]
# # 3 col: personal + cate + segment

# %%
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import os

print("--- B·∫ÆT ƒê·∫¶U QUY TR√åNH T·∫†O D·ªÆ LI·ªÜU NG·ªÆ C·∫¢NH (CONTEXT PIPELINE) ---")

# =============================================================================
# B∆Ø·ªöC 1: T√ÅI T·∫†O C√ÅC TR∆Ø·ªúNG GAP (TH√ìI QUEN MUA S·∫ÆM)
# =============================================================================
# L∆∞u √Ω: Ta l√†m tr√™n b·∫£n copy ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng lu·ªìng train model ch√≠nh
context_df = final_df.copy()

# S·∫Øp x·∫øp ƒë·ªÉ t√≠nh to√°n diff ch√≠nh x√°c
context_df.sort_values(by=['customer_unique_id', 'order_purchase_timestamp'], inplace=True)

# 1.1 T√≠nh Personal Avg Gap
# Kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn mua c·ªßa kh√°ch
context_df['diff_days'] = context_df.groupby('customer_unique_id')['order_purchase_timestamp'].diff().dt.days
# T√≠nh trung b√¨nh cho m·ªói kh√°ch
personal_gap_series = context_df.groupby('customer_unique_id')['diff_days'].mean()
# Fillna cho ng∆∞·ªùi mua l·∫ßn ƒë·∫ßu (b·∫±ng quantile 0.75 to√†n c·ª•c)
global_p75_gap = context_df['diff_days'].quantile(0.75)
personal_gap_series = personal_gap_series.fillna(global_p75_gap)

# 1.2 T√≠nh Category Avg Gap (LOGIC ƒê√É C·∫¢I TI·∫æN)
# Logic: T√≠nh gap d·ª±a tr√™n c√°c ƒê∆†N H√ÄNG DUY NH·∫§T (tr√°nh vi·ªác mua nhi·ªÅu m√≥n 1 ƒë∆°n l√†m gap = 0)
print("ƒêang t√≠nh to√°n Category Gap (Improved Logic)...")
unique_orders_per_cat = context_df.drop_duplicates(subset=['customer_unique_id', 'order_id', 'product_category_name_english']).copy()
unique_orders_per_cat.sort_values(by=['customer_unique_id', 'product_category_name_english', 'order_purchase_timestamp'], inplace=True)

# T√≠nh kho·∫£ng c√°ch gi·ªØa c√°c ƒë∆°n h√†ng trong c√πng danh m·ª•c c·ªßa kh√°ch
unique_orders_per_cat['cat_diff_days'] = unique_orders_per_cat.groupby(['product_category_name_english', 'customer_unique_id'])['order_purchase_timestamp'].diff().dt.days

# T√≠nh trung b√¨nh cho danh m·ª•c
category_gap_df = unique_orders_per_cat.groupby('product_category_name_english')['cat_diff_days'].mean().reset_index()
category_gap_df.rename(columns={'cat_diff_days': 'category_avg_gap'}, inplace=True)
# Fillna cho category
category_gap_df['category_avg_gap'] = category_gap_df['category_avg_gap'].fillna(category_gap_df['category_avg_gap'].mean())


# =============================================================================
# B∆Ø·ªöC 2: CUSTOMER SEGMENTATION (RFM + K-MEANS)
# =============================================================================
print("ƒêang th·ª±c hi·ªán ph√¢n kh√∫c kh√°ch h√†ng (RFM + K-Means)...")

# 2.1 Chu·∫©n b·ªã d·ªØ li·ªáu RFM
snapshot_date = context_df['order_purchase_timestamp'].max() + pd.Timedelta(days=1)

# T√≠nh RFM m·ª©c kh√°ch h√†ng (L√∫c n√†y customer_unique_id s·∫Ω l√† INDEX)
rfm_df = context_df.groupby('customer_unique_id').agg({
    'order_purchase_timestamp': lambda x: (snapshot_date - x.max()).days, # Recency
    'order_id': 'nunique', # Frequency
    'payment_value': 'sum' # Monetary
}).rename(columns={
    'order_purchase_timestamp': 'Recency',
    'order_id': 'Frequency',
    'payment_value': 'Monetary'
})

# 2.2 X·ª≠ l√Ω Outlier (IQR Method)
def get_non_outlier_index(df, cols):
    indices = df.index
    for col in cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        condition = (df[col] >= lower) & (df[col] <= upper)
        indices = indices.intersection(df[condition].index)
    return indices

clean_idx = get_non_outlier_index(rfm_df, ['Recency', 'Frequency', 'Monetary'])
rfm_clean = rfm_df.loc[clean_idx].copy()

print(f"D√πng {len(rfm_clean)} kh√°ch h√†ng (ƒë√£ l·ªçc outlier) ƒë·ªÉ hu·∫•n luy·ªán K-Means.")

# 2.3 Standard Scaling & K-Means
scaler = StandardScaler()
scaler.fit(rfm_clean[['Recency', 'Frequency', 'Monetary']])
rfm_scaled_all = scaler.transform(rfm_df[['Recency', 'Frequency', 'Monetary']])
rfm_clean_scaled = scaler.transform(rfm_clean[['Recency', 'Frequency', 'Monetary']])

kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
kmeans.fit(rfm_clean_scaled) # Fit tr√™n t·∫≠p s·∫°ch

# Predict cho TO√ÄN B·ªò kh√°ch h√†ng
rfm_df['segment'] = kmeans.predict(rfm_scaled_all)

# --- QUAN TR·ªåNG: RESET INDEX ƒê·ªÇ S·ª¨A L·ªñI KEYERROR ---
# ƒê∆∞a customer_unique_id t·ª´ Index ra th√†nh C·ªôt ƒë·ªÉ d√πng cho Analysis v√† Merge
rfm_df.reset_index(inplace=True) 

# --- PH√ÇN T√çCH CH√ÇN DUNG KH√ÅCH H√ÄNG ---
print("\n--- PH√ÇN T√çCH CH√ÇN DUNG KH√ÅCH H√ÄNG (CLUSTER ANALYSIS) ---")
segment_analysis = rfm_df.groupby('segment').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': 'mean',
    'customer_unique_id': 'count' # B√¢y gi·ªù c·ªôt n√†y ƒë√£ t·ªìn t·∫°i, code s·∫Ω ch·∫°y ƒë√∫ng
}).rename(columns={'customer_unique_id': 'Count'})

segment_analysis = segment_analysis.sort_values(by='Monetary', ascending=False)
print(segment_analysis.round(2))
print("-" * 60)
print("Ph√¢n ph·ªëi Segment:")
print(rfm_df['segment'].value_counts())


# =============================================================================
# B∆Ø·ªöC 3: MAPPING V√Ä XU·∫§T FILE ORDER LEVEL
# =============================================================================
print("ƒêang t·ªïng h·ª£p d·ªØ li·ªáu c·∫•p ƒë·ªô Order...")

# 3.1 Map Personal Gap
personal_gap_df = personal_gap_series.reset_index(name='personal_avg_gap')

# 3.2 Chu·∫©n b·ªã b·∫£ng ch√≠nh
output_df = context_df[['order_id', 'customer_unique_id', 'product_category_name_english']].copy()
# Gi·ªØ l·∫°i unique order + category
output_df.drop_duplicates(subset=['order_id', 'product_category_name_english'], keep='first', inplace=True)

# 3.3 Merge t·∫•t c·∫£
output_df = output_df.merge(personal_gap_df, on='customer_unique_id', how='left')
output_df = output_df.merge(category_gap_df, on='product_category_name_english', how='left')
# Merge Segment (B√¢y gi·ªù rfm_df ƒë√£ c√≥ c·ªôt customer_unique_id nh·ªù reset_index ·ªü tr√™n)
output_df = output_df.merge(rfm_df[['customer_unique_id', 'segment']], on='customer_unique_id', how='left')

# Fill NaN
output_df['personal_avg_gap'].fillna(global_p75_gap, inplace=True)
output_df['category_avg_gap'].fillna(output_df['category_avg_gap'].mean(), inplace=True)
output_df['segment'].fillna(-1, inplace=True)

# 3.4 L∆∞u file
output_filename = 'order_context_data.csv'
output_df.to_csv(output_filename, index=False)

print(f"\n‚úÖ ƒê√£ t·∫°o th√†nh c√¥ng file dataset b·ªï sung: {output_filename}")
print(f"Shape: {output_df.shape}")
print("5 d√≤ng ƒë·∫ßu ti√™n:")
print(output_df.head())

# %%
# ===== EXPORT CSV =====
output_filename = "order_context_data.csv"
output_df.to_csv(output_filename, index=False)
print(f"‚úî ƒê√£ xu·∫•t file: {output_filename}")

# ===== T·∫†O LINK T·∫¢I FILE (KAGGLE) =====
from IPython.display import FileLink
FileLink(output_filename)


# %% [markdown]
# # 2: Chu·∫©n b·ªã d·ªØ li·ªáu

# %%
# ==============================================================================
# B∆Ø·ªöC 1.2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU VƒÇN B·∫¢N (REVIEW TEXT)
# ==============================================================================
print("\n--- B·∫Øt ƒë·∫ßu B∆∞·ªõc 1.2: Chu·∫©n b·ªã D·ªØ li·ªáu VƒÉn b·∫£n ---")

# --- 1.2.1: T·ªïng h·ª£p b√¨nh lu·∫≠n cho m·ªói kh√°ch h√†ng ---
# Lo·∫°i b·ªè c√°c b√¨nh lu·∫≠n r·ªóng tr∆∞·ªõc khi join ƒë·ªÉ tr√°nh c√°c "[SEP]" th·ª´a
final_df['review_comment_message'] = final_df['review_comment_message'].str.strip()
final_df.replace('', np.nan, inplace=True)

agg_reviews = final_df.dropna(subset=['review_comment_message']).groupby('customer_unique_id')['review_comment_message'].apply(lambda x: " [SEP] ".join(x)).reset_index()
review_map = dict(zip(agg_reviews['customer_unique_id'], agg_reviews['review_comment_message']))

print(f"ƒê√£ t·ªïng h·ª£p ƒë∆∞·ª£c {len(review_map)} chu·ªói b√¨nh lu·∫≠n cho kh√°ch h√†ng.")

# >>> V√ç D·ª§ <<<
print("\n--- V√≠ d·ª• v·ªÅ c√°c b√¨nh lu·∫≠n ƒë√£ ƒë∆∞·ª£c t·ªïng h·ª£p ---")

# L·∫•y ra m·ªôt v√†i ID kh√°ch h√†ng c√≥ nhi·ªÅu h∆°n 1 b√¨nh lu·∫≠n ƒë·ªÉ xem v√≠ d·ª•
customer_counts = final_df.dropna(subset=['review_comment_message'])['customer_unique_id'].value_counts()
customers_with_multiple_reviews = customer_counts[customer_counts > 1].index.tolist()

num_examples_to_show = 3
if len(customers_with_multiple_reviews) >= num_examples_to_show:
    example_ids = customers_with_multiple_reviews[:num_examples_to_show]

    for i, customer_id in enumerate(example_ids):
        print(f"\n--- V√≠ d·ª• {i+1} ---")
        print(f"Customer ID: {customer_id}")

        # In ra c√°c b√¨nh lu·∫≠n g·ªëc
        original_comments = final_df[final_df['customer_unique_id'] == customer_id]['review_comment_message'].dropna().tolist()
        print("B√¨nh lu·∫≠n g·ªëc:")
        for comment in original_comments:
            print(f"  - \"{comment}\"")

        # In ra b√¨nh lu·∫≠n ƒë√£ ƒë∆∞·ª£c t·ªïng h·ª£p
        aggregated_comment = review_map.get(customer_id, "Kh√¥ng t√¨m th·∫•y b√¨nh lu·∫≠n t·ªïng h·ª£p.")
        print(f"B√¨nh lu·∫≠n ƒë√£ t·ªïng h·ª£p (ƒë·∫ßu v√†o cho m√¥ h√¨nh):")
        print(f"  -> \"{aggregated_comment}\"")
else:
    print("Kh√¥ng t√¨m th·∫•y ƒë·ªß kh√°ch h√†ng c√≥ nhi·ªÅu h∆°n 1 b√¨nh lu·∫≠n ƒë·ªÉ hi·ªÉn th·ªã v√≠ d·ª•.")

# # --- 1.2.2: Kh·ªüi t·∫°o Tokenizer ---
# MODEL_NAME = 'distilbert-base-uncased'
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# %% [markdown]
# # 3: Baseline: XGBoost + BERT

# %%
# ==============================================================================
# PH·∫¶N 0: C√ÄI ƒê·∫∂T V√Ä IMPORTS C√ÅC TH∆Ø VI·ªÜN C·∫¶N THI·∫æT
# ==============================================================================
# H·∫ßu h·∫øt ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t, ch·ªâ c·∫ßn th√™m xgboost
!pip install xgboost -q

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm.notebook import tqdm

# Gi·∫£ s·ª≠ 'final_df' ƒë√£ t·ªìn t·∫°i t·ª´ c√°c cell code tr∆∞·ªõc c·ªßa b·∫°n.
print("Ki·ªÉm tra shape c·ªßa final_df:", final_df.shape)

# ==============================================================================
# B∆Ø·ªöC 1: FEATURE ENGINEERING CHO M√î H√åNH TH√îNG TH∆Ø·ªúNG (XGBOOST)
# ==============================================================================
print("\n--- B·∫Øt ƒë·∫ßu B∆∞·ªõc 1: Feature Engineering cho XGBoost ---")

# --- 1.1: T·ªïng h·ª£p D·ªØ li·ªáu v·ªÅ C·∫•p ƒë·ªô Kh√°ch h√†ng ---
# Thay v√¨ gi·ªØ chu·ªói giao d·ªãch, ch√∫ng ta s·∫Ω t·ªïng h·ª£p ch√∫ng th√†nh c√°c ch·ªâ s·ªë th·ªëng k√™
# cho m·ªói kh√°ch h√†ng. ƒê√¢y l√† c√°ch l√†m kinh ƒëi·ªÉn.

# S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o c√°c ph√©p t√≠nh 'first' v√† 'last' l√† ch√≠nh x√°c
final_df = final_df.sort_values(by=['customer_unique_id', 'order_purchase_timestamp'])

# ƒê·ªãnh nghƒ©a c√°c h√†m t·ªïng h·ª£p
agg_funcs = {
    # ƒê·∫∑c tr∆∞ng T·∫ßn su·∫•t (Frequency)
    # 'order_id': 'count',
    # ƒê·∫∑c tr∆∞ng Ti·ªÅn t·ªá (Monetary)
    'payment_value': ['sum', 'mean', 'max', 'min'],
    # ƒê·∫∑c tr∆∞ng H√†nh vi
    'num_items': ['sum', 'mean'],
    'review_score': ['mean', 'min', 'std'],
    'delivery_days': ['mean', 'max'],
    'delivery_delay_days': ['mean', 'max'],
    # ƒê·∫∑c tr∆∞ng h·∫°ng m·ª•c (l·∫•y gi√° tr·ªã c·ªßa giao d·ªãch cu·ªëi c√πng)
    'customer_state': 'last',
    'product_category_name_english': 'last',
    'payment_type': 'last',
    # Nh√£n Churn (l·∫•y c·ªßa giao d·ªãch cu·ªëi c√πng)
    'churn': 'last'
}

customer_df = final_df.groupby('customer_unique_id').agg(agg_funcs)

# "L√†m ph·∫≥ng" t√™n c√°c c·ªôt ƒëa c·∫•p (MultiIndex)
customer_df.columns = ['_'.join(col).strip() for col in customer_df.columns.values]
# customer_df.rename(columns={'order_id_count': 'frequency'}, inplace=True)
customer_df.reset_index(inplace=True)

# --- 1.2: T·∫°o ƒê·∫∑c tr∆∞ng G·∫ßn ƒë√¢y (Recency) ---
# T√≠nh ng√†y cu·ªëi c√πng c·ªßa b·ªô d·ªØ li·ªáu ƒë·ªÉ l√†m m·ªëc
snapshot_date = final_df['order_purchase_timestamp'].max() + pd.DateOffset(days=1)
# T√≠nh ng√†y mua g·∫ßn nh·∫•t c·ªßa m·ªói kh√°ch h√†ng
recency_df = final_df.groupby('customer_unique_id')['order_purchase_timestamp'].max().reset_index()
recency_df.rename(columns={'order_purchase_timestamp': 'last_purchase_date'}, inplace=True)
# T√≠nh Recency (s·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua cu·ªëi)
recency_df['recency'] = (snapshot_date - recency_df['last_purchase_date']).dt.days

# Merge ƒë·∫∑c tr∆∞ng Recency v√†o dataframe kh√°ch h√†ng
# customer_df = pd.merge(customer_df, recency_df[['customer_unique_id', 'recency']], on='customer_unique_id')

print(f"DataFrame c·∫•p ƒë·ªô kh√°ch h√†ng ƒë∆∞·ª£c t·∫°o v·ªõi shape: {customer_df.shape}")


# ==============================================================================
# B∆Ø·ªöC 2: TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG VƒÇN B·∫¢N V·ªöI BERT
# ==============================================================================
print("\n--- B·∫Øt ƒë·∫ßu B∆∞·ªõc 2: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v·ªõi BERT ---")

# T·∫£i m√¥ h√¨nh v√† tokenizer (gi·ªëng nh∆∞ tr∆∞·ªõc)
MODEL_NAME = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
bert_model = AutoModel.from_pretrained(MODEL_NAME)

# Chuy·ªÉn m√¥ h√¨nh sang GPU ƒë·ªÉ tƒÉng t·ªëc
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model.to(device)
bert_model.eval() # ƒê·∫∑t ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√°

# S·ª≠ d·ª•ng l·∫°i review_map ƒë√£ t·∫°o ·ªü Giai ƒëo·∫°n 1 c·ªßa m√¥ h√¨nh ViT-like
# (N·∫øu ch∆∞a c√≥, b·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i ƒëo·∫°n code t·∫°o review_map)
print(f"S·ª≠ d·ª•ng l·∫°i {len(review_map)} chu·ªói b√¨nh lu·∫≠n ƒë√£ t·ªïng h·ª£p.")

def get_bert_embeddings(texts, model, tokenizer, device, batch_size=64):
    all_embeddings = []
    # Chia nh·ªè ƒë·ªÉ kh√¥ng l√†m qu√° t·∫£i GPU
    for i in tqdm(range(0, len(texts), batch_size), desc="T·∫°o BERT Embeddings"):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        ).to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        # L·∫•y embedding c·ªßa token [CLS]
        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        all_embeddings.extend(cls_embeddings)

    return np.array(all_embeddings)

# L·∫•y danh s√°ch ID v√† vƒÉn b·∫£n t∆∞∆°ng ·ª©ng t·ª´ customer_df
customers_with_reviews = customer_df[customer_df['customer_unique_id'].isin(review_map.keys())]
ids_to_process = customers_with_reviews['customer_unique_id'].tolist()
texts_to_process = [review_map[cid] for cid in ids_to_process]

# T·∫°o embeddings
bert_embeddings = get_bert_embeddings(texts_to_process, bert_model, tokenizer, device)

# T·∫°o DataFrame t·ª´ embeddings
bert_features_df = pd.DataFrame(bert_embeddings, columns=[f'bert_{i}' for i in range(bert_embeddings.shape[1])])
bert_features_df['customer_unique_id'] = ids_to_process

print(f"ƒê√£ t·∫°o {bert_embeddings.shape[0]} BERT embeddings v·ªõi {bert_embeddings.shape[1]} chi·ªÅu.")


# ==============================================================================
# B∆Ø·ªöC 3: K·∫æT H·ª¢P D·ªÆ LI·ªÜU V√Ä CHIA T·∫¨P
# ==============================================================================
print("\n--- B·∫Øt ƒë·∫ßu B∆∞·ªõc 3: K·∫øt h·ª£p v√† chia d·ªØ li·ªáu ---")

# Merge ƒë·∫∑c tr∆∞ng BERT v√†o dataframe kh√°ch h√†ng (s·ª≠ d·ª•ng left join)
final_customer_df = pd.merge(customer_df, bert_features_df, on='customer_unique_id', how='left')
# ƒêi·ªÅn 0 cho nh·ªØng kh√°ch h√†ng kh√¥ng c√≥ review
final_customer_df.fillna(0, inplace=True)

# M√£ h√≥a One-Hot cho c√°c c·ªôt h·∫°ng m·ª•c c√≤n l·∫°i
final_customer_df = pd.get_dummies(final_customer_df, columns=['customer_state_last', 'product_category_name_english_last', 'payment_type_last'])

print(f"DataFrame cu·ªëi c√πng c√≥ shape: {final_customer_df.shape}")

# Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh Scikit-Learn
X = final_customer_df.drop(columns=['customer_unique_id', 'churn_last'])
y = final_customer_df['churn_last']

# Chia d·ªØ li·ªáu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"K√≠ch th∆∞·ªõc t·∫≠p Train: {X_train.shape}")
print(f"K√≠ch th∆∞·ªõc t·∫≠p Test: {X_test.shape}")


# ==============================================================================
# B∆Ø·ªöC 4: HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å M√î H√åNH XGBOOST
# ==============================================================================
print("\n--- B·∫Øt ƒë·∫ßu B∆∞·ªõc 4: Hu·∫•n luy·ªán v√† ƒê√°nh gi√° XGBoost ---")

# Kh·ªüi t·∫°o m√¥ h√¨nh XGBoost
# scale_pos_weight h·ªØu √≠ch cho d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng
scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    scale_pos_weight=scale_pos_weight,
    random_state=42
)

# Hu·∫•n luy·ªán m√¥ h√¨nh
xgb_model.fit(X_train, y_train)

# D·ª± ƒëo√°n tr√™n t·∫≠p test
y_pred = xgb_model.predict(X_test)
y_pred_proba = xgb_model.predict_proba(X_test)[:, 1] # X√°c su·∫•t cho l·ªõp 1 (churn)

# ƒê√°nh gi√° hi·ªáu su·∫•t
print("\n--- K·∫æT QU·∫¢ C·ª¶A M√î H√åNH XGBOOST + BERT ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
# Thay roc_auc_score(y_test, y_pred) b·∫±ng roc_auc_score(y_test, y_pred_proba)
print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")

# V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost + BERT) on Test Set')
plt.show()

# %% [markdown]
# # 4: ƒê√°nh gi√°

# %%
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Assume X_test, y_test, and xgb_model are already defined and available
# from the previous cell where XGBoost was trained.

print("--- ƒê√°nh gi√° M√¥ h√¨nh XGBoost (Kh√¥ng c√≥ BERT Embeddings) ---")

# D·ª± ƒëo√°n tr√™n t·∫≠p test
y_pred_xgb_only = xgb_model.predict(X_test)
y_pred_proba_xgb_only = xgb_model.predict_proba(X_test)[:, 1] # X√°c su·∫•t cho l·ªõp 1 (churn)

# ƒê√°nh gi√° hi·ªáu su·∫•t
print("\n--- K·∫æT QU·∫¢ C·ª¶A M√î H√åNH XGBOOST TR√äN T·∫¨P TEST ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_xgb_only):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred_proba_xgb_only):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred_xgb_only):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_xgb_only):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_xgb_only):.4f}")

# V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n
cm_xgb_only = confusion_matrix(y_test, y_pred_xgb_only)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb_only, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (XGBoost Only) on Test Set')
plt.show()

# %% [markdown]
# # 5: SHAP cho XGB

# %%
# ==============================================================================
# B∆Ø·ªöC XAI: S·ª¨ D·ª§NG SHAP ƒê·ªÇ ƒêI·ªÄU TRA DATA LEAKAGE
# ==============================================================================
!pip install shap -q
import shap

print("\n--- B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch XAI v·ªõi SHAP ---")

# --- B∆∞·ªõc 1: Kh·ªüi t·∫°o Explainer ---
# Ch√∫ng ta c·∫ßn m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán (xgb_model) v√† d·ªØ li·ªáu hu·∫•n luy·ªán (X_train)
# TreeExplainer l√† m·ªôt thu·∫≠t to√°n t·ªëi ∆∞u c·ªßa SHAP d√†nh ri√™ng cho c√°c m√¥ h√¨nh c√¢y.
explainer = shap.TreeExplainer(xgb_model)

print("ƒê√£ kh·ªüi t·∫°o SHAP TreeExplainer.")

# --- B∆∞·ªõc 2: T√≠nh to√°n gi√° tr·ªã SHAP tr√™n t·∫≠p Test ---
# Vi·ªác t√≠nh to√°n n√†y c√≥ th·ªÉ m·∫•t m·ªôt ch√∫t th·ªùi gian
print("ƒêang t√≠nh to√°n gi√° tr·ªã SHAP cho t·∫≠p Test...")
shap_values = explainer.shap_values(X_test)

print("ƒê√£ t√≠nh to√°n xong gi√° tr·ªã SHAP.")


# --- B∆∞·ªõc 3: Tr·ª±c quan h√≥a v√† Ph√¢n t√≠ch ---

# 3.1. Bi·ªÉu ƒë·ªì T√≥m t·∫Øt (Summary Plot) - QUAN TR·ªåNG NH·∫§T
# Bi·ªÉu ƒë·ªì n√†y x·∫øp h·∫°ng c√°c ƒë·∫∑c tr∆∞ng theo t·∫ßm quan tr·ªçng trung b√¨nh c·ªßa ch√∫ng
# v√† cho th·∫•y ·∫£nh h∆∞·ªüng c·ªßa ch√∫ng (gi√° tr·ªã cao/th·∫•p c·ªßa ƒë·∫∑c tr∆∞ng ·∫£nh h∆∞·ªüng ƒë·∫øn d·ª± ƒëo√°n nh∆∞ th·∫ø n√†o).
print("\n--- Bi·ªÉu ƒë·ªì T√≥m t·∫Øt T·∫ßm quan tr·ªçng c·ªßa ƒê·∫∑c tr∆∞ng (SHAP Summary Plot) ---")
print("Bi·ªÉu ƒë·ªì n√†y cho th·∫•y c√°c ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t (tr√™n c√πng) v√† t√°c ƒë·ªông c·ªßa ch√∫ng.")
print("M√†u ƒë·ªè: Gi√° tr·ªã ƒë·∫∑c tr∆∞ng cao. M√†u xanh: Gi√° tr·ªã ƒë·∫∑c tr∆∞ng th·∫•p.")
print("Tr·ª•c x: Gi√° tr·ªã SHAP. Gi√° tr·ªã d∆∞∆°ng -> tƒÉng kh·∫£ nƒÉng churn. Gi√° tr·ªã √¢m -> gi·∫£m kh·∫£ nƒÉng churn.")

shap.summary_plot(shap_values, X_test, plot_type="dot")


# 3.2. Bi·ªÉu ƒë·ªì T·∫ßm quan tr·ªçng (Bar Plot)
# M·ªôt c√°ch nh√¨n kh√°c, ƒë∆°n gi·∫£n h∆°n v·ªÅ t·∫ßm quan tr·ªçng trung b√¨nh c·ªßa c√°c ƒë·∫∑c tr∆∞ng.
print("\n--- Bi·ªÉu ƒë·ªì T·∫ßm quan tr·ªçng Trung b√¨nh (SHAP Bar Plot) ---")
shap.summary_plot(shap_values, X_test, plot_type="bar")


# --- B∆∞·ªõc 4: Ph√¢n t√≠ch k·∫øt qu·∫£ t·ª´ bi·ªÉu ƒë·ªì ---
print("\n--- PH√ÇN T√çCH K·∫æT QU·∫¢ XAI ---")
print("H√£y quan s√°t c√°c bi·ªÉu ƒë·ªì tr√™n, ƒë·∫∑c bi·ªát l√† Summary Plot (bi·ªÉu ƒë·ªì ƒë·∫ßu ti√™n):")
print("\n1. ƒê·∫∑c tr∆∞ng n√†o ƒë·ª©ng ƒë·∫ßu danh s√°ch?")
print("   -> R·∫•t c√≥ th·ªÉ b·∫°n s·∫Ω th·∫•y 'recency' v√† 'frequency' ·ªü c√°c v·ªã tr√≠ cao nh·∫•t. ƒêi·ªÅu n√†y x√°c nh·∫≠n ch√∫ng l√† nh·ªØng y·∫øu t·ªë d·ª± ƒëo√°n m·∫°nh nh·∫•t.")
print("\n2. Ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa 'recency':")
print("   -> Nh√¨n v√†o d·∫£i m√†u c·ªßa 'recency'. B·∫°n s·∫Ω th·∫•y c√°c ƒëi·ªÉm m√†u ƒë·ªè (recency cao) n·∫±m ho√†n to√†n ·ªü ph√≠a b√™n ph·∫£i c·ªßa tr·ª•c x (gi√° tr·ªã SHAP d∆∞∆°ng), c√≥ nghƒ©a l√† 'recency' cao ƒë·∫©y m·∫°nh d·ª± ƒëo√°n churn. ƒêi·ªÅu n√†y kh·ªõp ho√†n h·∫£o v·ªõi logic g√°n nh√£n c·ªßa b·∫°n v√† l√† b·∫±ng ch·ª©ng m·∫°nh m·∫Ω v·ªÅ data leakage.")
print("\n3. Ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa 'frequency':")
print("   -> Nh√¨n v√†o d·∫£i m√†u c·ªßa 'frequency'. R·∫•t c√≥ th·ªÉ b·∫°n s·∫Ω th·∫•y c√°c ƒëi·ªÉm m√†u xanh (frequency th·∫•p, v√≠ d·ª• = 1) n·∫±m ·ªü ph√≠a b√™n ph·∫£i (tƒÉng kh·∫£ nƒÉng churn, khi k·∫øt h·ª£p v·ªõi recency cao), trong khi c√°c ƒëi·ªÉm m√†u ƒë·ªè (frequency cao) n·∫±m ·ªü b√™n tr√°i (gi·∫£m kh·∫£ nƒÉng churn).")
print("\n==> K·∫æT LU·∫¨N T·ª™ XAI: C√°c bi·ªÉu ƒë·ªì SHAP ƒë√£ tr·ª±c quan h√≥a m·ªôt c√°ch r√µ r√†ng m·ªëi quan h·ªá to√°n h·ªçc gi·ªØa c√°c ƒë·∫∑c tr∆∞ng nh∆∞ 'recency', 'frequency' v√† nh√£n 'churn' m√† ch√∫ng ta ƒë√£ nghi ng·ªù. ƒê√¢y l√† b·∫±ng ch·ª©ng kh√¥ng th·ªÉ ch·ªëi c√£i v·ªÅ vi·ªác r√≤ r·ªâ logic (logical leakage) trong pipeline d·ªØ li·ªáu ban ƒë·∫ßu.")

# %% [markdown]
# # 6: L∆∞u c√°i file v·ªÅ

# %%
import joblib
import pickle
import os

# --- 1. X√ÅC ƒê·ªäNH C√ÅC "T√ÄI S·∫¢N" C·∫¶N L∆ØU ---
model_to_save = xgb_model
model_columns_to_save = X_train.columns.tolist()

# --- 2. L∆ØU FILE V√ÄO TH∆Ø M·ª§C OUTPUT C·ª¶A KAGGLE ---
# M·∫∑c ƒë·ªãnh th∆∞ m·ª•c l√†m vi·ªác hi·ªán t·∫°i l√† /kaggle/working/
model_filename = 'churn_model.joblib'
# columns_filename = 'model_columns.pkl'

# 2.1. L∆∞u m√¥ h√¨nh
try:
    joblib.dump(model_to_save, model_filename)
    print(f"ƒê√£ l∆∞u th√†nh c√¥ng m√¥ h√¨nh t·∫°i: {os.path.abspath(model_filename)}")
except Exception as e:
    print(f"L·ªói khi l∆∞u m√¥ h√¨nh: {e}")

# # 2.2. L∆∞u danh s√°ch c·ªôt
# try:
#     with open(columns_filename, 'wb') as f:
#         pickle.dump(model_columns_to_save, f)
#     print(f"ƒê√£ l∆∞u th√†nh c√¥ng danh s√°ch c·ªôt t·∫°i: {os.path.abspath(columns_filename)}")
# except Exception as e:
#     print(f"L·ªói khi l∆∞u danh s√°ch c·ªôt: {e}")

print("\nC√°c file ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c Output (/kaggle/working).")
print("B·∫°n c√≥ th·ªÉ t√¨m th·∫•y ch√∫ng trong ph·∫ßn 'Output' c·ªßa Kernel sau khi ch·∫°y xong.")

# %%
!pip freeze | grep -E 'scikit-learn|xgboost|pandas|numpy|shap|joblib|pickle'


