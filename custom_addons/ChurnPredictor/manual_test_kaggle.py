import os
import json
import shutil
import sys
import time
import zipfile
from datetime import datetime  # <--- M·ªöI: D√πng ƒë·ªÉ l·∫•y th·ªùi gian

# --- [FIX QUAN TR·ªåNG] √âP BU·ªòC D√ôNG UTF-8 ---
os.environ['PYTHONUTF8'] = '1'

try:
    from kaggle.api.kaggle_api_extended import KaggleApi
except ImportError:
    print("‚ùå L·ªói: Ch∆∞a c√†i th∆∞ vi·ªán kaggle. Vui l√≤ng ch·∫°y: pip install kaggle")
    sys.exit(1)

# ==============================================================================
# C·∫§U H√åNH
# ==============================================================================
KAGGLE_USERNAME = 'subinkhang'
DATASET_SLUG = 'subinkhang/olist-merged-dataset-2016-2017'
KERNEL_SLUG = 'subinkhang/churn-predictor-4'
NOTEBOOK_FILE_NAME = "churn-predictor-4.ipynb"
TEMP_DIR = r'D:\ChurnPredictor\temp_kaggle_process'
SAMPLE_CSV_NAME = "olist_merged_2018_month_01.csv"

# ==============================================================================
# LOGIC
# ==============================================================================

def get_paths():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config_dir = os.path.join(current_dir, 'config')
    data_dir = os.path.join(current_dir, 'data', 'sample')
    return {
        'config': config_dir,
        'kaggle_json': os.path.join(config_dir, 'kaggle.json'),
        'sample_data': os.path.join(data_dir, SAMPLE_CSV_NAME)
    }

def init_kaggle_api(paths):
    if not os.path.exists(paths['kaggle_json']):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file kaggle.json!")
        sys.exit(1)
    os.environ['KAGGLE_CONFIG_DIR'] = paths['config']
    try:
        api = KaggleApi()
        api.authenticate()
        return api
    except Exception as e:
        print(f"‚ùå L·ªói x√°c th·ª±c: {e}")
        sys.exit(1)

def prepare_temp_dir():
    if os.path.exists(TEMP_DIR):
        try:
            shutil.rmtree(TEMP_DIR)
        except: pass
    dataset_dir = os.path.join(TEMP_DIR, 'dataset')
    kernel_dir = os.path.join(TEMP_DIR, 'kernel')
    os.makedirs(dataset_dir, exist_ok=True)
    os.makedirs(kernel_dir, exist_ok=True)
    return dataset_dir, kernel_dir

def upload_dataset_with_trigger_info(api, dataset_dir, sample_file_path):
    print("[3] B·∫Øt ƒë·∫ßu Upload Dataset...")
    
    # 1. T·∫£i d·ªØ li·ªáu c≈© v·ªÅ ƒë·ªÉ g·ªôp
    print(f"   -> ƒêang t·∫£i d·ªØ li·ªáu hi·ªán t·∫°i t·ª´ {DATASET_SLUG}...")
    try:
        api.dataset_download_files(DATASET_SLUG, path=dataset_dir, unzip=True)
    except: pass

    # X√≥a file zip th·ª´a
    for item in os.listdir(dataset_dir):
        if item.endswith(".zip"): os.remove(os.path.join(dataset_dir, item))

    # 2. Copy file CSV m·ªõi v√†o
    if not os.path.exists(sample_file_path):
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file CSV m·ªõi: {sample_file_path}")
        sys.exit(1)
    
    target_csv = os.path.join(dataset_dir, SAMPLE_CSV_NAME)
    shutil.copy(sample_file_path, target_csv)
    print(f"   -> ƒê√£ th√™m file: {SAMPLE_CSV_NAME}")

    # =================================================================
    # [M·ªöI] T·ª∞ ƒê·ªòNG T·∫†O FILE TRIGGER INFO
    # =================================================================
    run_id = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    trigger_data = {
        "run_id": run_id,
        "triggered_by": "Local Script (Odoo)",
        "new_file_name": SAMPLE_CSV_NAME,
        "action": "append_and_retrain"
    }
    
    # L∆∞u file json n√†y v√†o c√πng th∆∞ m·ª•c dataset s·∫Øp upload
    trigger_file_path = os.path.join(dataset_dir, 'trigger_info.json')
    
    with open(trigger_file_path, 'w', encoding='utf-8') as f:
        json.dump(trigger_data, f, indent=4)
        
    print(f"   -> üÜî ƒê√£ t·∫°o file c·∫•u h√¨nh ch·∫°y: Run ID [{run_id}]")
    # =================================================================

    # 3. T·∫°o metadata cho Dataset
    meta_data = {
        "title": "Olist Merged Dataset 2016-2017",
        "id": DATASET_SLUG,
        "licenses": [{"name": "CC0-1.0"}]
    }
    with open(os.path.join(dataset_dir, 'dataset-metadata.json'), 'w', encoding='utf-8') as f:
        json.dump(meta_data, f, indent=4)

    # 4. Upload l√™n Kaggle
    try:
        print("   -> ƒêang ƒë·ªìng b·ªô l√™n Kaggle...")
        api.dataset_create_version(
            folder=dataset_dir, 
            version_notes=f'Trigger Run ID: {run_id}', 
            dir_mode='zip', 
            quiet=False
        )
        print("‚úÖ Upload Dataset th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªñI UPLOAD: {e}")
        sys.exit(1)

def fix_notebook_encoding_for_windows(notebook_path):
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            content = json.load(f)
        with open(notebook_path, 'w', encoding='utf-8') as f:
            json.dump(content, f, indent=4, ensure_ascii=True)
    except: pass

def check_kernel_status(api):
    print("   -> üì° ƒêang ki·ªÉm tra tr·∫°ng th√°i Kernel...")
    try:
        # L·∫•y tr·∫°ng th√°i
        status = api.kernels_status(KERNEL_SLUG)
        # status l√† object, √©p ki·ªÉu v·ªÅ string ho·∫∑c truy c·∫≠p thu·ªôc t√≠nh
        # L∆∞u √Ω: Th∆∞ vi·ªán kaggle tr·∫£ v·ªÅ object KernelStatus
        s_val = str(status).split(" ")[0] # L·∫•y t·ª´ ƒë·∫ßu ti√™n (v√≠ d·ª•: "running"...)
        
        print(f"   -> üî• Tr·∫°ng th√°i tr√™n Kaggle: {status}")
    except: 
        print("   -> (Kh√¥ng l·∫•y ƒë∆∞·ª£c tr·∫°ng th√°i chi ti·∫øt, nh∆∞ng l·ªánh ƒë√£ g·ª≠i ƒëi)")

def trigger_kernel(api, kernel_dir):
    print("[4] B·∫Øt ƒë·∫ßu Trigger Notebook...")
    
    try:
        api.kernels_pull(KERNEL_SLUG, path=kernel_dir, metadata=False)
    except: pass 
    
    downloaded_notebook = os.path.join(kernel_dir, NOTEBOOK_FILE_NAME)
    slug_name = KERNEL_SLUG.split('/')[-1] + ".ipynb"
    possible_file = os.path.join(kernel_dir, slug_name)

    if not os.path.exists(downloaded_notebook):
        if os.path.exists(possible_file):
            os.rename(possible_file, downloaded_notebook)
        else:
            files = [f for f in os.listdir(kernel_dir) if f.endswith('.ipynb')]
            if files: os.rename(os.path.join(kernel_dir, files[0]), downloaded_notebook)

    fix_notebook_encoding_for_windows(downloaded_notebook)

    meta_data = {
        "id": KERNEL_SLUG,
        "title": "Churn Predictor 4",
        "code_file": NOTEBOOK_FILE_NAME,
        "language": "python",
        "kernel_type": "notebook",
        "is_private": "false",
        "enable_gpu": "true",
        "enable_internet": "true",
        "dataset_sources": [DATASET_SLUG],
        "kernel_sources": []
    }
    
    with open(os.path.join(kernel_dir, 'kernel-metadata.json'), 'w', encoding='utf-8') as f:
        json.dump(meta_data, f, indent=4)

    print("   -> ƒêang k√≠ch ho·∫°t ch·∫°y l·∫°i (Push)...")
    try:
        api.kernels_push(folder=kernel_dir)
        print("‚úÖ L·ªánh Push th√†nh c√¥ng.")
        
        time.sleep(3)
        check_kernel_status(api)
        
    except Exception as e:
        print(f"‚ùå L·ªñI TRIGGER: {e}")
        sys.exit(1)

if __name__ == "__main__":
    print("--- START PYTHON API TEST (AUTO JSON CREATION) ---")
    paths = get_paths()
    api = init_kaggle_api(paths)
    d_dir, k_dir = prepare_temp_dir()
    
    # G·ªçi h√†m m·ªõi ƒë√£ t√≠ch h·ª£p t·∫°o file json
    upload_dataset_with_trigger_info(api, d_dir, paths['sample_data'])
    
    print("‚è≥ ƒê·ª£i 10 gi√¢y ƒë·ªÉ Kaggle x·ª≠ l√Ω dataset...")
    time.sleep(10)
    
    trigger_kernel(api, k_dir)
    print("\n--- HO√ÄN T·∫§T ---")